# ðŸ” Transformer from Scratch (PyTorch)

This repository contains a complete implementation of the **Transformer architecture** from scratch using **PyTorch**, based on the original paper:  
ðŸ“„ *"Attention Is All You Need" â€“ Vaswani et al., 2017*

The project demonstrates a foundational understanding of modern NLP architectures by building each component manually â€” including **multi-head attention**, **positional encoding**, **encoder-decoder blocks**, and **masking**.

> âœ… Built entirely from first principles, without using high-level libraries like `torch.nn.Transformer`.

---

## ðŸš€ Whatâ€™s Implemented

- [x] Scaled Dot-Product Attention  
- [x] Multi-Head Attention  
- [x] Positional Encoding  
- [x] Transformer Encoder & Decoder  
- [x] Masking (padding + subsequent)  
- [x] Training loop with sample data (optional: toy task like translation)  
- [x] Inference example

---

## ðŸ§  Highlights

- Pure implementation using **PyTorch tensors and modules**
- Modular code â€” easy to read, test, and extend
- Closely follows the structure of the original Transformer paper
- Can serve as a **learning tool** or base for further research

---

## ðŸ“‚ Project Structure


