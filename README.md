# 🔁 Transformer from Scratch (PyTorch)

This repository contains a complete implementation of the **Transformer architecture** from scratch using **PyTorch**, based on the original paper:  
📄 *"Attention Is All You Need" – Vaswani et al., 2017*

The project demonstrates a foundational understanding of modern NLP architectures by building each component manually — including **multi-head attention**, **positional encoding**, **encoder-decoder blocks**, and **masking**.

> ✅ Built entirely from first principles, without using high-level libraries like `torch.nn.Transformer`.

---

## 🚀 What’s Implemented

- [x] Scaled Dot-Product Attention  
- [x] Multi-Head Attention  
- [x] Positional Encoding  
- [x] Transformer Encoder & Decoder  
- [x] Masking (padding + subsequent)  
- [x] Training loop with sample data (optional: toy task like translation)  
- [x] Inference example

---

## 🧠 Highlights

- Pure implementation using **PyTorch tensors and modules**
- Modular code — easy to read, test, and extend
- Closely follows the structure of the original Transformer paper
- Can serve as a **learning tool** or base for further research

---

## 📂 Project Structure


